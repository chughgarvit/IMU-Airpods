# -*- coding: utf-8 -*-
"""Percom baselines.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIcQjEXBC4iqDwSLsWFnJUvsTLBVOm75

Earbit
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
from scipy.fft import fft, fftfreq, ifft
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

def butterworth_filter(signal_axis, cutoff=20.0, fs=50, order=5):

    nyquist = 0.5 * fs
    normalized_cutoff = cutoff / nyquist
    b, a = butter(order, normalized_cutoff, btype='low', analog=False)
    return filtfilt(b, a, signal_axis)

def segment_data(signals, labels, sampling_rate=50, window_size_s=30, step_size_s=1):

    window_len = int(window_size_s * sampling_rate)
    step_len   = int(step_size_s * sampling_rate)

    N = len(signals)
    windows = []
    window_labels = []
    start_indices = []

    idx = 0
    while idx + window_len <= N:
        segment = signals[idx:idx+window_len, :]
        segment_labels = labels[idx:idx+window_len]
        windows.append(segment)
        window_labels.append(segment_labels)
        start_indices.append(idx)
        idx += step_len

    return windows, window_labels, start_indices

def compute_rms(signal_axis):
    return np.sqrt(np.mean(signal_axis**2))

def compute_variance(signal_axis):
    return np.var(signal_axis)

def compute_entropy(signal_axis, bin_size=10, min_val=-50, max_val=50):
    clipped = np.clip(signal_axis, min_val, max_val)
    bins = np.arange(min_val, max_val+bin_size, bin_size)
    hist, _ = np.histogram(clipped, bins=bins, density=True)
    hist = hist + 1e-9  # avoid log(0)
    return -np.sum(hist * np.log(hist))

def compute_peak_power(signal_axis, fs=50):
    n = len(signal_axis)
    freqs = fftfreq(n, d=1/fs)
    fft_vals = np.abs(fft(signal_axis))
    # Consider only positive frequencies
    pos_mask = freqs > 0
    freqs = freqs[pos_mask]
    fft_vals = fft_vals[pos_mask]
    if len(fft_vals) == 0:
        return 0.0
    return np.max(fft_vals)

def compute_power_spectral_density(signal_axis, fs=50):

    n = len(signal_axis)
    fft_vals = np.abs(fft(signal_axis))**2
    # log scale
    psd = np.sum(fft_vals) + 1e-9
    return np.log(psd)

def compute_zero_crossing(signal_axis):

    zero_crosses = np.where(np.diff(np.signbit(signal_axis)))[0]
    return len(zero_crosses)

def compute_zero_crossing_variance(signal_axis):

    zero_crosses = np.where(np.diff(np.signbit(signal_axis)))[0]
    if len(zero_crosses) < 2:
        return 0.0
    intervals = np.diff(zero_crosses)
    return np.var(intervals)

def compute_peak_frequency(signal_axis, fs=50):
    n = len(signal_axis)
    freqs = fftfreq(n, d=1/fs)
    fft_vals = np.abs(fft(signal_axis))
    pos_mask = freqs > 0
    freqs = freqs[pos_mask]
    fft_vals = fft_vals[pos_mask]
    if len(fft_vals) == 0:
        return 0.0
    max_idx = np.argmax(fft_vals)
    return freqs[max_idx]

def autocorrelation(signal_axis):
    signal_axis = signal_axis - np.mean(signal_axis)
    N = len(signal_axis)
    freq_domain = fft(signal_axis, n=2*N)
    power_spectrum = freq_domain * np.conjugate(freq_domain)
    autocorr = ifft(power_spectrum).real
    autocorr = autocorr[:N]

    autocorr /= np.max(np.abs(autocorr)) + 1e-9
    return autocorr

def count_autocorr_peaks(ac):

    peaks = 0
    for i in range(1, len(ac)-1):
        if ac[i] > ac[i-1] and ac[i] > ac[i+1]:
            peaks += 1
    return peaks

def count_prominent_peaks(ac, threshold=0.25):

    peaks = 0
    for i in range(1, len(ac)-1):
        if (ac[i] - ac[i-1] > threshold) and (ac[i] - ac[i+1] > threshold):
            peaks += 1
    return peaks

def count_weak_peaks(ac, threshold=0.25):

    peaks = 0
    for i in range(1, len(ac)-1):
        if (ac[i-1] - ac[i] > threshold) and (ac[i+1] - ac[i] > threshold):
            peaks += 1
    return peaks

def max_autocorr_value(ac):
    return np.max(ac)

def first_autocorr_peak(ac):

    for i in range(1, len(ac)-1):
        if ac[i] > ac[i-1] and ac[i] > ac[i+1]:
            return ac[i]
    return 0.0

def extract_features_earbit(window, fs=50):

    feats = []
    def extract_features(axis_data):
        f13 = []
        f13.append(compute_rms(axis_data))        # RMS
        f13.append(compute_variance(axis_data))   # variance
        f13.append(compute_entropy(axis_data))    # entropy
        f13.append(compute_peak_power(axis_data, fs=fs))       # peak power
        f13.append(compute_power_spectral_density(axis_data, fs=fs))  # PSD

        f13.append(compute_zero_crossing(axis_data))           # zero crossing
        f13.append(compute_zero_crossing_variance(axis_data))  # var of zero cross
        f13.append(compute_peak_frequency(axis_data, fs=fs))   # peak freq

        ac = autocorrelation(axis_data)
        num_peaks = count_autocorr_peaks(ac)
        f13.append(num_peaks)                                  # # autocorr peaks
        f13.append(count_prominent_peaks(ac))                  # # prominent peaks
        f13.append(count_weak_peaks(ac))                       # # weak peaks
        f13.append(max_autocorr_value(ac))                     # max autocorr
        f13.append(first_autocorr_peak(ac))                    # first peak
        return f13

    ear_x = window[:, 0]
    ear_y = window[:, 1]
    ear_z = window[:, 2]
    neck_x = window[:, 3]
    neck_y = window[:, 4]
    neck_z = window[:, 5]

    for axis_data in [ear_x, ear_y, ear_z, neck_x, neck_y, neck_z]:
        feats.extend(extract_features(axis_data))

    return np.array(feats)


def create_feature_matrix(windows, window_labels, fs=50):

    X = []
    y = []
    for w, lbl_set in zip(windows, window_labels):

        for c in range(w.shape[1]):
            w[:, c] = butterworth_filter(w[:, c], cutoff=20.0, fs=fs, order=5)


        if np.mean(lbl_set) >= 0.5:
            window_label = 1
        else:
            window_label = 0

        feats = extract_features_earbit(w, fs=fs)
        X.append(feats)
        y.append(window_label)
    return np.vstack(X), np.array(y)


def train_random_forest(X, y):

    clf = RandomForestClassifier(random_state=42)
    clf.fit(X, y)
    return clf

def sliding_inference(clf, windows, fs=50):

    probs = []
    for w in windows:
        for c in range(w.shape[1]):
            w[:, c] = butterworth_filter(w[:, c], cutoff=20.0, fs=fs, order=5)
        feats = extract_features_earbit(w, fs=fs).reshape(1, -1)
        p = clf.predict_proba(feats)
        probs.append(p[0, 1])
    return np.array(probs)

def moving_average_filter(probabilities, window_size=35):

    smoothed = np.zeros_like(probabilities)
    half_w = window_size // 2
    for i in range(len(probabilities)):
        left = max(0, i - half_w)
        right = min(len(probabilities), i + half_w + 1)
        smoothed[i] = np.mean(probabilities[left:right])
    return smoothed

def threshold_decision(probabilities, thresh=0.5):

    return (probabilities >= thresh).astype(int)

def aggregate_events(binary_preds, start_indices, step_sec=1, window_sec=30, merge_gap_sec=600, min_event_sec=120):

    events = []
    in_event = False
    event_start = None

    for i, val in enumerate(binary_preds):
        t_s = start_indices[i] / 50.0  # since we used 50 Hz in data generation
        if val == 1 and not in_event:
            in_event = True
            event_start = t_s
        elif val == 0 and in_event:
            in_event = False
            event_end = t_s + window_sec  # approximate
            events.append((event_start, event_end))

    if in_event:
        events.append((event_start, start_indices[-1]/50.0 + window_sec))

    merged_events = []
    if len(events) > 0:
        current_start, current_end = events[0]
        for i in range(1, len(events)):
            s, e = events[i]
            if s - current_end <= merge_gap_sec:
                current_end = e
            else:
                merged_events.append((current_start, current_end))
                current_start, current_end = s, e
        merged_events.append((current_start, current_end))
    else:
        merged_events = []

    final_events = []
    for (s, e) in merged_events:
        duration = e - s
        if duration >= min_event_sec:
            final_events.append((s, e))

    return final_events

def main():

    windows, window_labels, start_indices = segment_data(
        signals, labels,
        sampling_rate=50,
        window_size_s=30,
        step_size_s=1
    )
    print(f"[INFO] #windows: {len(windows)}")

    X, y = create_feature_matrix(windows, window_labels, fs=50)
    print(f"[INFO] Feature matrix shape: {X.shape}, #pos: {np.sum(y)}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )
    rf = train_random_forest(X_train, y_train)
    acc = rf.score(X_test, y_test)
    print(f"[RESULT] RF Accuracy on test set: {acc:.3f}")

    raw_probs = sliding_inference(rf, windows, fs=50)

    smoothed_probs = moving_average_filter(raw_probs, window_size=35)
    bin_preds = threshold_decision(smoothed_probs, thresh=0.5)

    events = aggregate_events(
        bin_preds, start_indices,
        step_sec=1,
        window_sec=30,
        merge_gap_sec=600,   # 10 minutes
        min_event_sec=120    # 2 minutes
    )
    print("[INFO] Detected 'eating events' after aggregation:")
    for i, (start, end) in enumerate(events):
        print(f"  -> Event {i+1}: Start={start:.1f}s, End={end:.1f}s, Duration={end-start:.1f}s")

if __name__ == "__main__":
    main()

"""IMChew"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
from scipy.fft import fft, fftfreq
from python_speech_features import mfcc
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def segment_signal(signals, labels, window_size_s=3, sampling_rate=60):

    window_length = window_size_s * sampling_rate
    num_samples = len(signals)

    window_length = int(window_length)

    windowed_data = []
    windowed_labels = []

    start_idx = 0
    while start_idx + window_length <= num_samples:
        window = signals[start_idx:start_idx+window_length, :]
        window_label = labels[start_idx:start_idx+window_length]
        if np.mean(window_label) >= 0.5:
            final_label = 1
        else:
            final_label = 0

        windowed_data.append(window)
        windowed_labels.append(final_label)

        start_idx += window_length

    return windowed_data, windowed_labels


def compute_power(signal_axis):
    return np.mean(signal_axis**2)


def butter_bandpass_filter(signal_axis, lowcut=0.1, highcut=3.0, fs=60, order=2):

    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band', analog=False)
    filtered = filtfilt(b, a, signal_axis)
    return filtered

def spectral_centroid(signal_axis, fs=60):

    N = len(signal_axis)
    freqs = fftfreq(N, d=1/fs)
    magnitudes = np.abs(fft(signal_axis))

    positive_indices = np.where(freqs > 0)
    freqs = freqs[positive_indices]
    magnitudes = magnitudes[positive_indices]

    if np.sum(magnitudes) < 1e-8:
        return 0.0
    return np.sum(freqs * magnitudes) / np.sum(magnitudes)


def extract_features_from_window(window, fs=60):

    features = []
    window_length = window.shape[0]

    for axis in range(6):
        axis_data = window[:, axis]
        mean_val = np.mean(axis_data)
        var_val  = np.var(axis_data)
        power_val = compute_power(axis_data)
        features.extend([mean_val, var_val, power_val])

    for axis in range(6):
        axis_data = window[:, axis]
        sc = spectral_centroid(axis_data, fs=fs)
        features.append(sc)

    for axis in range(6):
        axis_data = window[:, axis]
        axis_data = axis_data + 1e-8
        mfcc_feat = mfcc(
            axis_data, samplerate=fs, numcep=12, winlen=1.0, winstep=1.0,
            nfft=1024, preemph=0.0
        )
        avg_mfcc = np.mean(mfcc_feat, axis=0)
        features.extend(list(avg_mfcc))

    return np.array(features)


def create_feature_matrix(windowed_data, windowed_labels, fs=60):

    feature_list = []
    for window in windowed_data:
        feats = extract_features_from_window(window, fs=fs)
        feature_list.append(feats)
    X = np.vstack(feature_list)
    y = np.array(windowed_labels)
    return X, y

def train_classifiers(X_train, y_train):
    classifiers = {}

    # Logistic Regression
    lr = LogisticRegression(random_state=42, max_iter=500)
    lr.fit(X_train, y_train)
    classifiers['LogisticRegression'] = lr

    # Decision Tree
    dt = DecisionTreeClassifier(random_state=42, max_depth=10)
    dt.fit(X_train, y_train)
    classifiers['DecisionTree'] = dt

    # Random Forest
    rf = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=10)
    rf.fit(X_train, y_train)
    classifiers['RandomForest'] = rf

    return classifiers


def classify_window(window, classifier, fs=60):
    feats = extract_features_from_window(window, fs=fs).reshape(1, -1)
    pred = classifier.predict(feats)
    return int(pred[0])


def aggregate_chewing_episodes(predictions, min_consecutive_nonchew=3):
    episodes = []
    in_episode = False
    start_idx = 0
    consecutive_nonchew = 0

    for i, p in enumerate(predictions):
        if not in_episode:
            if p == 1:
                in_episode = True
                start_idx = i
                consecutive_nonchew = 0
        else:
            if p == 0:
                consecutive_nonchew += 1
                if consecutive_nonchew >= min_consecutive_nonchew:
                    end_idx = i - min_consecutive_nonchew
                    episodes.append((start_idx, end_idx))
                    in_episode = False
            else:
                consecutive_nonchew = 0

    if in_episode:
        end_idx = len(predictions) - 1
        episodes.append((start_idx, end_idx))

    return episodes

def count_chews_in_episode(windowed_data, episode, fs=60):

    (start_idx, end_idx) = episode
    episode_signal = np.concatenate(windowed_data[start_idx:end_idx+1], axis=0)
    chunk_size = 10 * fs
    chunk_size = int(chunk_size)
    total_chews = 0

    i = 0
    while i + chunk_size <= len(episode_signal):
        chunk = episode_signal[i:i+chunk_size, :]
        chunk_axis = chunk[:, 0]

        filtered = butter_bandpass_filter(chunk_axis, lowcut=0.1, highcut=3.0, fs=fs, order=2)
        N = len(filtered)
        freqs = fftfreq(N, d=1/fs)
        magnitudes = np.abs(fft(filtered))

        valid_freqs = []
        valid_mags = []
        for f, m in zip(freqs, magnitudes):
            if 0.5 <= f <= 2.5:
                valid_freqs.append(f)
                valid_mags.append(m)

        if len(valid_mags) > 0:
            max_idx = np.argmax(valid_mags)
            dominant_freq = valid_freqs[max_idx]
        else:
            dominant_freq = 0.0

        chunk_duration_s = 10.0  # we used 10-second chunk
        chunk_chews = dominant_freq * chunk_duration_s

        total_chews += int(round(chunk_chews))

        i += chunk_size

    return total_chews

def main():

    windowed_data, windowed_labels = segment_signal(signals, labels,
                                                    window_size_s=3,
                                                    sampling_rate=60)
    print(f"[INFO] Number of windows: {len(windowed_data)}")

    X, y = create_feature_matrix(windowed_data, windowed_labels, fs=60)
    print(f"[INFO] Feature matrix shape: {X.shape}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    classifiers = train_classifiers(X_train, y_train)

    for name, clf in classifiers.items():
        acc = clf.score(X_test, y_test)
        print(f"[RESULT] {name} test accuracy: {acc:.3f}")

    chosen_clf = classifiers['RandomForest']

    predictions = []
    for w in windowed_data:
        p = classify_window(w, chosen_clf, fs=60)
        predictions.append(p)

    episodes = aggregate_chewing_episodes(predictions, min_consecutive_nonchew=3)
    print(f"[INFO] Detected chewing episodes: {episodes}")

    for idx, ep in enumerate(episodes):
        chew_count = count_chews_in_episode(windowed_data, ep, fs=60)
        print(f"   -> Episode {idx+1}: Windows {ep}, Estimated {chew_count} chews")

if __name__ == "__main__":
    main()

"""EarBit"""

